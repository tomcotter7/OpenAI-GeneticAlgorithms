\chapter{Summary and Reflections}

\label{ch:summary}

\section{Summary of Work}

The project successfully tested both the outcome of using neuro-evolution on Space Invaders and the transfer of that learning into other similar Atari game environments. Whilst the project did not produce the results I had hoped for, the confirmation that RL algorithms can in fact transfer their learning, I still produced a detailed and well thought out project. The main restriction for this project was the time-constrains, which proved difficult to stick to especially given the time it takes to train the agents via neuro-evolution. Furthermore, agents trained in this way can only produce outputs that correspond to discrete actions. Many RL tasks nowadays use a continuous action space, such as the ShadowHand environments from OpenAI. These are robotics hands that can perform simple tasks, which could in theory be trained using neuroevolution, but their action space is continuous. Apart from these two limitations, the project was successful, I learnt a lot more about the frameworks for RL tasks and gained more experience in Tensorflow, which I enjoyed.

\section{Future Work}


This project did not show evidence of transfer learning being significantly viable in RL tasks, when neuro-evolution is used. However, when transfer learning is utilised in computer vision tasks, it is often a base understanding of the image that is transferred, and the rest of the layers are then retrained to fit the new problem. This could be viable in RL tasks too. If I had more time, I would like to compare using neuro-evolution on a environment with two initial generations, the first one is a random set of models, and the second is the final generation from neuro-evolution on a second similar environment. We could then see if the two generations converge to an optima at different rates, perhaps the one with the previously learnt knowledge converging faster due to its previous knowledge of how to move about the environment. I would also like to test the transfer learning capabilities of different types of algorithms, such as DDPG or TD3 to compare with neuro-evolution.
