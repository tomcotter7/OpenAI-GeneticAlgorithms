\chapter{Evaluation}

\label{ch:eval}

\section{Training on Space Invaders}

Using the determined operators and rates, the final set of models were trained. In this case, the environment was experienced 20 times per agent per generation, and the lives were no longer limited to 1 - the agent would play until the game was finished. This was tested for 30 generations, and the population contained 20 agents. The result can be seen in Fig. N. The final mean score for the agents was x.

\section{Transfer Learning}

The comparison between an agent performing random actions and the best agent taken from the final generation from the Space Invaders training for both new environments can be seen in Fig. N. From these results we can say that -

**POSITIVE**

Transfer learning provides a significant improvement for basic RL learning tasks. In Demon Attack, utilising transfer learning resulted in a x\% improvement over using a random agent, which is a significant improvement. In Carnival, which is slightly further from Space Invaders than Demon Attack, transfer learning resulted in a y\% improvement over using a random agent. This is very positive for both new environments.

**NEGATIVE**

Utilising transfer learning for RL does not produce a significant improvement over using a random agent. In both Demon Attack and Carnival, the transfer learned agents did not perform significantly better than a random agent to say that transfer learning is useful. In this case, we can also test evolving the set of trained agents on a new environment for 10 generations and comparing this to starting with random agents. These results can be seen in Fig. M. From these results, we can say that - 




**IF PREVIOUS TEST DOESN'T SHOW SIGNIFICANT IMPROVEMENT THEN DO THE HOW MANY GENERATIONS TO CONVERGE TEST**
